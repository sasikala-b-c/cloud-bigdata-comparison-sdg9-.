# Methodology

- Dataset: synthetic CSV generated locally to avoid large downloads; sized 1â€“5 GB.
- Workload: PySpark ETL with joins, window aggregations, and optional MLlib stage.
- Metrics captured: runtime, rows processed, shuffle read/write, partitions, and environment metadata.
- Cost estimation: instance on-demand rates x runtime (approximate). Record in metrics.csv.
- Elasticity & manageability: qualitative rubric filled during runs (setup complexity, logs/metrics, autoscaling behavior).
- Sustainability signals: region energy proxy, spot/preemptible usage.
